{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Error Filters\n",
    "\n",
    "This notebook implements the most basic prediction error filter, which is constructed to predict observed training data $d_i^\\text{obs}$ for $i=0, ..., N-1$ using $n$ earlier data $d_{i-j-1}^\\text{obs}$ for $j=0, ..., n-1$ via the linear relation (discrete convolution)\n",
    "\n",
    "\\begin{equation}\n",
    "d_i = \\sum_{j=0}^{n-1} d_{i-j-1}^\\text{obs} m_j\\,.\n",
    "\\end{equation}\n",
    "\n",
    "The task of finding the $n$ filter coefficients $m_j$ can be formulated as an inverse problem. Assuming that the observational errors are Gaussian with covariance matrix $\\mathbf{C}$, the maximum-likelihood set of coefficients minimises the least-squares misfit functional\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi(\\mathbf{m}) = \\frac{1}{2} \\sum_{i,j=0}^{N-1} (d_i - d_i^\\text{obs}) C_{ij}^{-1} (d_j - d_j^\\text{obs})\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Inserting the forward modelling equation and forcing the derivative of $\\chi$ to zero, yields the normal equations for the maximum-likelihood coefficients $\\hat{\\mathbf{m}}$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i,j=0}^{N-1} d_i^\\text{obs} C_{ij}^{-1} d_{j-q-1}^\\text{obs} = \\sum_{k=0}^{n-1} \\left( \\sum_{i,j=0}^{N-1} d_{i-k-1}^\\text{obs} C_{ij}^{-1} d_{j-q-1}^\\text{obs} \\right)\\,\\hat{m}_k\\,.\n",
    "\\end{equation}\n",
    "\n",
    "This can be written in vector-matrix form,\n",
    "\n",
    "\\begin{equation}\n",
    "b_q = \\sum_{k=0}^{n-1} A_{qk} \\hat{m}_k\\,,\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "b_q = \\sum_{i,j=0}^{N-1} d_i^\\text{obs} C_{ij}^{-1} d_{j-q-1}^\\text{obs}\\,,\\qquad A_{qk}=\\sum_{i,j=0}^{N-1} d_{i-k-1}^\\text{obs} C_{ij}^{-1} d_{j-q-1}^\\text{obs}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "From this we see that the left-hand side and the matrix contain auto-correlations of the observations. Hence, linear predictions are made on the basis of the correlation properties of the data.\n",
    "\n",
    "The data needed for this notebook can be downloaded with this Polybox link: https://polybox.ethz.ch/index.php/s/a3EDhinFdATu4qt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Packages and setup\n",
    "\n",
    "Import the necessary Python packages and add a few lines to embellish figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy.signal.filter import bandpass\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times\"\n",
    "plt.rcParams.update({'font.size': 65})\n",
    "plt.rcParams['xtick.major.pad']='12'\n",
    "plt.rcParams['ytick.major.pad']='12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General input\n",
    "\n",
    "Basic input, including the file to be read and the average spacing between channels."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# AVALANCHE DATA ========================================\n",
    "\n",
    "# Input file.\n",
    "input_file='/Users/andreas/Desktop/PEF/av3009_raw.npy'\n",
    "# Scale for plotting.\n",
    "scale=7\n",
    "# Receiver spacing.\n",
    "dx=2.0\n",
    "# Time increment.\n",
    "dt=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRIMSVÃ–TN DATA ========================================\n",
    "\n",
    "# Input file.\n",
    "input_file='/Users/andreas/Desktop/PEF/2021-05-26T00_02_14.998000Z.npy'\n",
    "# Scale for plotting.\n",
    "scale=4\n",
    "# Receiver spacing.\n",
    "dx=8.0\n",
    "# Time increment.\n",
    "dt=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimun and maximum trace indices for plotting.\n",
    "ix_min=150\n",
    "ix_max=500\n",
    "\n",
    "# Minimum time index and length of dataset for plotting.\n",
    "i0=3900\n",
    "Nplot=400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data reading and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Read and plot complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cct=np.load(input_file)\n",
    "print(cct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt=cct.shape[1]\n",
    "nx=cct.shape[0]-1\n",
    "\n",
    "t=np.linspace(0.0,nt*dt,nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace normalisation for plotting.\n",
    "normalisation=200.0\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "for i in np.arange(ix_min,ix_max):\n",
    "        \n",
    "    data = cct[i,i0:i0+Nplot]/normalisation     \n",
    "    dist_var = (i-1)*dx\n",
    "    \n",
    "    plt.plot(t[i0:i0+Nplot],(scale*data)+dist_var,'k-', alpha = 0.4)\n",
    "    plt.fill_between(t[i0:i0+Nplot],(scale*data)+dist_var,y2=np.ones(np.shape(t[i0:i0+Nplot]))*dist_var,where=(data+dist_var>=dist_var), interpolate=True,fc='k',alpha=0.8)\n",
    "\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('distance [m]')\n",
    "plt.title('raw data',pad=30)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Filtering and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and maximum frequencies [Hz].\n",
    "freqmin=1.0\n",
    "freqmax=49.0\n",
    "# Downsampling factor.\n",
    "downsample=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency-domain filtering.\n",
    "cct_filt=np.zeros(np.shape(cct))\n",
    "for i in range(cct.shape[0]-1): cct_filt[i,:]=bandpass(cct[i,:],freqmin=freqmin,freqmax=freqmax,df=1.0/dt,corners=4,zerophase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling.\n",
    "cct_filt_down=cct_filt[:,0:nt:downsample]\n",
    "\n",
    "nx=cct_filt_down.shape[0]-1\n",
    "nt=cct_filt_down.shape[1]\n",
    "dt=downsample*dt\n",
    "\n",
    "t=np.linspace(0.0,nt*dt,nt)\n",
    "\n",
    "i0=int(i0/downsample)\n",
    "Nplot=int(Nplot/downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace normalisation for plotting.\n",
    "normalisation=200.0\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "for i in np.arange(ix_min,ix_max):\n",
    "        \n",
    "    data = cct_filt_down[i,i0:i0+Nplot]/normalisation     \n",
    "    dist_var = (i-1)*dx\n",
    "    \n",
    "    plt.plot(t[i0:i0+Nplot],(scale*data)+dist_var,'k-', alpha = 0.4)\n",
    "    plt.fill_between(t[i0:i0+Nplot],(scale*data)+dist_var,y2=np.ones(np.shape(t[i0:i0+Nplot]))*dist_var,where=(data+dist_var>=dist_var), interpolate=True,fc='k',alpha=0.8)\n",
    "\n",
    "plt.xlabel('time [s]',labelpad=20)\n",
    "plt.ylabel('distance [m]',labelpad=20)\n",
    "plt.title('filtered data',pad=30)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "filename='OUTPUT/filtered_data.png'\n",
    "plt.savefig(filename,dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Pick a specific trace\n",
    "\n",
    "As data, we first pick one of the available traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the trace.\n",
    "i_trace=340\n",
    "# Pick trace.\n",
    "d=cct_filt_down[i_trace,:]\n",
    "\n",
    "# Plot individual trace.\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(d,'k')\n",
    "plt.grid()\n",
    "plt.xlabel('sample index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Covariance matrix\n",
    "\n",
    "We first need an estimate of the data covariance matrix $\\mathbf{C}$. For this, we first need to determine the size of the training dataset $N$. We then take a window of this length and slide it across the noise part of the signal, prior to the first wave arrivals. From this, we get average cross-correlations between pairs of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points in the training dataset.\n",
    "Nd=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation vector.\n",
    "corr=np.zeros(Nd)\n",
    "\n",
    "# Minimum and maximum indices from the noise time series.\n",
    "imin=100\n",
    "imax=3900\n",
    "\n",
    "# Compute mean and remove from data.\n",
    "mean=np.mean(d[imin:imax])\n",
    "d-=mean\n",
    "\n",
    "# Compute correlation vector.\n",
    "for i in np.arange(imin,imax):\n",
    "    for j in range(Nd):\n",
    "        corr[j]+=d[i]*d[i+j]\n",
    "\n",
    "corr/=np.float(imax-imin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(corr,'k')\n",
    "plt.grid()\n",
    "plt.title('average cross-correlation',pad=20)\n",
    "plt.xlabel('sample index')\n",
    "plt.xlim([0,Nd-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally build $\\mathbf{C}$ from the average cross-correlation by putting shifted copies of it into the rows of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.zeros([Nd,Nd])\n",
    "for i in range(Nd): C[i,:]=np.roll(corr,i)\n",
    "Cinv=np.linalg.inv(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prediction\n",
    "\n",
    "We start the simple prediction step by choosing a part of the time series that contains actual signal. Based on this, we build the vector $\\mathbf{b}$ and the matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of filter coefficients (model parameters)\n",
    "n=88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting index of the time series of interest (training dataset).\n",
    "i0=4100\n",
    "\n",
    "# Plot the training dataset.\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(np.arange(-n,Nd),d[i0-n:i0+Nd],'--k')\n",
    "plt.plot(np.arange(-n,Nd),d[i0-n:i0+Nd],'ko',MarkerSize=12)\n",
    "plt.plot(np.arange(0,Nd),d[i0:i0+Nd],'k',LineWidth=2)\n",
    "plt.grid()\n",
    "plt.xlim([-n,Nd])\n",
    "plt.title('training dataset',pad=20)\n",
    "plt.xlabel('sample index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we assemble $\\mathbf{b}$ and $\\mathbf{A}$. To make this more efficient, we use the fact that the matrix $X_{i,q}=\\sum_{j=0}^{N} C_{ij}^{-1} d_{j-q-1}^\\text{obs}$ appears in both $\\mathbf{b}$ and $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the auxiliary matrix X.\n",
    "X=np.zeros([Nd,n])\n",
    "for q in range(n): X[:,q]=np.dot(Cinv,d[(i0-q-1):(i0-q-1+Nd)])\n",
    "    \n",
    "# Build vector b.\n",
    "b=np.dot(d[i0:(i0+Nd)],X)\n",
    "\n",
    "# Build matrix A.\n",
    "A=np.zeros([n,n])\n",
    "for k in range(n): A[:,k]=np.dot(d[(i0-k-1):(i0-k-1+Nd)],X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve linear system.\n",
    "p=np.dot(np.linalg.inv(A),b)\n",
    "print(p)\n",
    "\n",
    "# Plot filter coefficients.\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(p,'k',LineWidth=2)\n",
    "plt.plot(p,'ko',MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlabel('coefficient index i')\n",
    "plt.ylabel('filter coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Predicting the training dataset\n",
    "\n",
    "Trying to predict the training dataset helps to check if the algorithm works at all, and it allows us to choose the proper number of model parameters that roughly produces an rms error of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the prediction.\n",
    "d_pred=np.zeros(Nd)\n",
    "\n",
    "for j in range(Nd):\n",
    "    for i in range(n):\n",
    "        d_pred[j]+=d[i0+j-i-1]*p[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of observed and predicted data.\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.plot(np.arange(-n,Nd),d[i0-n:i0+Nd],'--k')\n",
    "plt.plot(np.arange(-n,Nd),d[i0-n:i0+Nd],'ko',MarkerSize=12)\n",
    "plt.plot(np.arange(0,Nd),d[i0:i0+Nd],'k',LineWidth=4)\n",
    "plt.plot(np.arange(0,Nd),d_pred,c=[0.75,0.75,0.75],LineWidth=4)\n",
    "plt.plot(np.arange(0,Nd),d_pred,'o',c=[0.5,0.5,0.5],LineWidth=1,MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlim([-n-1,Nd])\n",
    "plt.title('observation/prediction comparison',pad=30)\n",
    "plt.xlabel('sample index',labelpad=20)\n",
    "plt.ylabel('nanostrain / s', labelpad=20)\n",
    "plt.tight_layout()\n",
    "filename='OUTPUT/training_data_prediction.png'\n",
    "plt.savefig(filename,dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(np.arange(0,Nd),d[i0:i0+Nd]-d_pred,'k',LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd),d[i0:i0+Nd]-d_pred,'ko',MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlim([-n-1,Nd])\n",
    "plt.title('difference observation - prediction',pad=30)\n",
    "plt.xlabel('sample index', labelpad=20)\n",
    "plt.ylabel('nanostrain / s', labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how good the prediction actually is, we compute the rms error using the previously estimated covariance matrix $\\mathbf{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residual.\n",
    "res=np.dot(d_pred-d[i0:(i0+Nd)],Cinv)\n",
    "res=np.dot(res,d_pred-d[i0:(i0+Nd)])\n",
    "res=np.sqrt(res/np.float(Nd))\n",
    "print('rms error: %f' % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Predicting beyond the training dataset\n",
    "\n",
    "To see how useful the model parameters are to predict more than just the training dataset, we extend this time series to more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New number of samples.\n",
    "Nd_new=2*Nd\n",
    "\n",
    "# Compute the prediction.\n",
    "d_pred=np.zeros(Nd_new)\n",
    "\n",
    "for j in range(Nd_new):\n",
    "    for i in range(n):\n",
    "        d_pred[j]+=d[i0+j-i-1]*p[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of observed and predicted data.\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.plot(np.arange(-n,Nd_new),d[i0-n:i0+Nd_new],'--k')\n",
    "plt.plot(np.arange(-n,Nd_new),d[i0-n:i0+Nd_new],'ko',MarkerSize=12)\n",
    "plt.plot(np.arange(0,Nd_new),d[i0:i0+Nd_new],'k',LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd_new),d_pred,c=[0.75,0.75,0.75],LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd_new),d_pred,'o',c=[0.75,0.75,0.75],MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlim([-n,Nd_new])\n",
    "plt.title('observation/prediction comparison',pad=30)\n",
    "plt.xlabel('sample index',labelpad=20)\n",
    "plt.ylabel('nanostrain / s', labelpad=20)\n",
    "plt.tight_layout()\n",
    "filename='OUTPUT/beyond_training_data_prediction.png'\n",
    "plt.savefig(filename,dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.plot(np.arange(0,Nd_new),d[i0:i0+Nd_new]-d_pred,'k',LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd_new),d[i0:i0+Nd_new]-d_pred,'ko',MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlim([-n,Nd_new])\n",
    "plt.title('difference observation - prediction',pad=30)\n",
    "plt.xlabel('sample index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation vector.\n",
    "corr_new=np.zeros(Nd_new)\n",
    "\n",
    "# Minimum and maximum indices from the noise time series.\n",
    "imin=100\n",
    "imax=3900\n",
    "\n",
    "# Compute correlation vector.\n",
    "for i in np.arange(imin,imax):\n",
    "    for j in range(Nd_new):\n",
    "        corr_new[j]+=d[i]*d[i+j]\n",
    "\n",
    "corr_new/=np.float(imax-imin)\n",
    " \n",
    "# Make the new correlation matrix.\n",
    "C_new=np.zeros([Nd_new,Nd_new])\n",
    "for i in range(Nd_new): C_new[i,:]=np.roll(corr_new,i)\n",
    "Cinv_new=np.linalg.inv(C_new)\n",
    "\n",
    "# Compute residual.\n",
    "res=np.dot(d_pred-d[i0:(i0+Nd_new)],Cinv_new)\n",
    "res=np.dot(res,d_pred-d[i0:(i0+Nd_new)])\n",
    "res=np.sqrt(res/np.float(Nd_new))\n",
    "print('rms error: %f' % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Predicting a different time series\n",
    "\n",
    "Now we use the same model parameters but for predicting a different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the trace.\n",
    "i_trace_new=350\n",
    "# Pick trace.\n",
    "d_new=cct_filt_down[i_trace_new,:]\n",
    "\n",
    "# Plot individual trace.\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(d_new,'k')\n",
    "plt.grid()\n",
    "plt.xlabel('sample index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the prediction.\n",
    "d_pred=np.zeros(Nd_new)\n",
    "\n",
    "for j in range(Nd_new):\n",
    "    for i in range(n):\n",
    "        d_pred[j]+=d_new[i0+j-i-1]*p[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of observed and predicted data.\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.plot(np.arange(-n,Nd_new),d_new[i0-n:i0+Nd_new],'--k')\n",
    "plt.plot(np.arange(-n,Nd_new),d_new[i0-n:i0+Nd_new],'ko',MarkerSize=12)\n",
    "plt.plot(np.arange(0,Nd_new),d_new[i0:i0+Nd_new],'k',LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd_new),d_pred,c=[0.75,0.75,0.75],LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd_new),d_pred,'o',c=[0.75,0.75,0.75],MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlim([-n,Nd_new])\n",
    "plt.title('observation/prediction comparison',pad=30)\n",
    "plt.xlabel('sample index',labelpad=20)\n",
    "plt.ylabel('nanostrain / s', labelpad=20)\n",
    "plt.tight_layout()\n",
    "filename='OUTPUT/other_data_prediction.png'\n",
    "plt.savefig(filename,dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.plot(np.arange(0,Nd_new),d_new[i0:i0+Nd_new]-d_pred,'k',LineWidth=2)\n",
    "plt.plot(np.arange(0,Nd_new),d_new[i0:i0+Nd_new]-d_pred,'ko',MarkerSize=12)\n",
    "plt.grid()\n",
    "plt.xlim([-n,Nd_new])\n",
    "plt.title('difference observation - prediction',pad=30)\n",
    "plt.xlabel('sample index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation vector.\n",
    "corr_new=np.zeros(Nd_new)\n",
    "\n",
    "# Minimum and maximum indices from the noise time series.\n",
    "imin=100\n",
    "imax=3900\n",
    "\n",
    "# Compute correlation vector.\n",
    "for i in np.arange(imin,imax):\n",
    "    for j in range(Nd_new):\n",
    "        corr_new[j]+=d_new[i]*d_new[i+j]\n",
    "\n",
    "corr_new/=np.float(imax-imin)\n",
    " \n",
    "# Make the new correlation matrix.\n",
    "C_new=np.zeros([Nd_new,Nd_new])\n",
    "for i in range(Nd_new): C_new[i,:]=np.roll(corr_new,i)\n",
    "Cinv_new=np.linalg.inv(C_new)\n",
    "\n",
    "# Compute residual.\n",
    "res=np.dot(d_pred-d_new[i0:(i0+Nd_new)],Cinv_new)\n",
    "res=np.dot(res,d_pred-d_new[i0:(i0+Nd_new)])\n",
    "res=np.sqrt(res/np.float(Nd_new))\n",
    "print('rms error: %f' % res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
